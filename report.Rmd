---
title: "R Notebook"
output: html_notebook
---

The data I chose is:
Hotels
- Aim is to predict if a booking will cancel or not
- Target variable: is_canceled
- Download: https://www.louisaslett.com/Courses/MISCADA/hotels.csv

```{r}
download.file("https://www.louisaslett.com/Courses/MISCADA/hotels.csv", "hotels.csv")
hotels.original <- readr::read_csv("hotels.csv")
```

```{r}
library("skimr")
library("tidyverse")
library("readr")
library("ggplot2")
library("dplyr")
```

*****

# Data exploration

```{r}
View(hotels.original)
```

```{r}
skim(hotels.original)
```

```{r}
DataExplorer::plot_bar(hotels.original)
```

```{r}
DataExplorer::plot_histogram(hotels.original)
```
```{r}
DataExplorer::plot_density(hotels.original)
```

```{r}
DataExplorer::plot_boxplot(hotels.original, by = "is_canceled", ncol = 3)
```
```{r}
months_ordered <- c("January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December")
hotels.original$arrival_date_month <- factor(hotels.original$arrival_date_month, levels = months_ordered, ordered = TRUE)

monthly_cancellation_rate <- hotels.original %>%
  group_by(arrival_date_month) %>%
  summarise(cancel_rate = mean(is_canceled, na.rm = TRUE)) %>%
  filter(!is.na(cancel_rate))

ggplot(monthly_cancellation_rate, aes(x = arrival_date_month, y = cancel_rate)) +
  geom_col(fill = "steelblue") +
  theme_minimal() +
  labs(x = "Month", y = "Cancellation Rate", title = "Monthly Cancellation Rate") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```



*****

```{r}
hotels <- hotels.original |>
  select(-reservation_status, -reservation_status_date, -assigned_room_type, -country, -agent, -company)
```


```{r}
hotels <- hotels |>
  mutate(kids = case_when(
    children + babies > 0 ~ "kids",
    TRUE ~ "none"
  )) |>
  select(-babies, -children) |>
  mutate(parking = case_when(
    required_car_parking_spaces > 0 ~ "parking",
    TRUE ~ "none"
  )) |>
  select(-required_car_parking_spaces) |>
  mutate(is_canceled = ifelse(is_canceled == 0, "no", "yes")) |>
  mutate(total_nights = stays_in_weekend_nights+stays_in_week_nights) |>
  select(-stays_in_weekend_nights, -stays_in_week_nights)
  
```

```{r}
hotels <- hotels %>% mutate_if(is.character, as.factor)
```

*****


```{r}
library("mlr3")
library("mlr3learners")
library("data.table")
library("mlr3verse")
library("ranger")
library("xgboost")
library(mlr3misc)


```


```{r}
set.seed(212) # set seed for reproducibility

iscanceled_task <- TaskClassif$new(id = "iscancled",
                               backend = hotels,
                               target = "is_canceled",
                               positive = "yes")
```


```{r}
cv5 <- rsmp("cv", folds = 5)

cv5$instantiate(iscanceled_task)
```


```{r}
lrn_baseline <- lrn("classif.featureless", predict_type = "prob") 

lrn_cart     <- lrn("classif.rpart", predict_type = "prob") 

lrn_log_reg  <- lrn("classif.log_reg", predict_type = "prob") 

lrn_lda <- lrn("classif.lda", predict_type = "prob") 

lrn_xgboost  <- lrn("classif.xgboost", predict_type = "prob") 

```


```{r}
pl_missing <- po("fixfactors") %>>%
  po("removeconstants") %>>%
  po("imputesample", affect_columns = selector_type(c("ordered", "factor"))) %>>%
  po("imputemean")

pl_log_reg <- pl_missing %>>% po(lrn_log_reg)
```


```{r}
pl_xgb <- po("encode") %>>% po(lrn_xgboost)
```


```{r}

res <- benchmark(data.table(
  task       = list(iscanceled_task),
  learner    = list(lrn_baseline,
                    lrn_cart,
                    pl_log_reg,
                    lrn_lda,
                    pl_xgb
                    
                    ),
  resampling = list(cv5)
  ), 
  store_models = TRUE
)


res_aggregated_1 <- res$aggregate(list(msr("classif.ce"),
                   msr("classif.acc"),
                   msr("classif.auc"),
                   msr("classif.fpr"), 
                   msr("classif.fnr")))

print(res_aggregated_1)


```



```{r} 

library(mlr3)
library(mlr3tuning)
library(mlr3learners)
library(paradox)
library(mlr3pipelines)



graph <- po("encode") %>>% po("learner", learner = lrn("classif.xgboost", predict_type = "prob"))
learner <- GraphLearner$new(graph)

param_set <- ParamSet$new(list(
  ParamDbl$new("classif.xgboost.eta", lower = 0.01, upper = 0.3), # 注意这里的调整
  ParamInt$new("classif.xgboost.max_depth", lower = 3, upper = 10),
  ParamDbl$new("classif.xgboost.subsample", lower = 0.5, upper = 1),
  ParamDbl$new("classif.xgboost.colsample_bytree", lower = 0.5, upper = 1)
))


cv5 <- rsmp("cv", folds = 5)
cv5$instantiate(iscanceled_task)


instance <- TuningInstanceSingleCrit$new(
  task = iscanceled_task,
  learner = learner,
  resampling = cv5,
  measure = msr("classif.auc"),
  search_space = param_set,
  terminator = trm("evals", n_evals = 50)
)

tuner$optimize(instance)


instance$result 
```

            
```{r}
instance$result


instance$result$classif.xgboost.eta


instance$result$classif.xgboost.max_depth


instance$result$classif.xgboost.subsample

instance$result$classif.xgboost.colsample_bytree


library(mlr3)
library(mlr3tuning)
library(mlr3learners)
library(mlr3pipelines)
library(mlr3measures)

learner_xgb <- lrn("classif.xgboost", predict_type = "prob",eta = 0.05078901, max_depth = 9,subsample = 0.5031509,colsample_bytree = 0.9683127)


pl_xgb_best <- po("encode") %>>% po("learner", learner = learner_xgb, id = "xgb_best")


res <- benchmark(data.table(
  task       = list(iscanceled_task),
  learner    = list( lrn_baseline,lrn_cart,pl_log_reg,lrn_lda,pl_xgb,pl_xgb_best),
  resampling = list(cv5)
), store_models = TRUE)

res_aggregated <- res$aggregate(list(
  msr("classif.ce"),  
  msr("classif.acc"),
  msr("classif.auc"), 
  msr("classif.fpr"), 
  msr("classif.fnr") 
))

print(res_aggregated)




```






